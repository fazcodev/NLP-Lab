{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "corpus = \"I am salman khan am salman\"\n",
    "\n",
    "# Tokenize and lowercase\n",
    "tokens = [word.lower() for word in word_tokenize(corpus)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "from nltk import bigrams\n",
    "\n",
    "# Get all bigrams\n",
    "bigram_list = list(bigrams(tokens))\n",
    "\n",
    "# Calculate frequency of each bigram\n",
    "bigram_freq = Counter(bigram_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Get unique words in the corpus\n",
    "words = list(set(tokens))\n",
    "\n",
    "# Create a frequency distribution for single words\n",
    "word_freq = FreqDist(tokens)\n",
    "\n",
    "# Initialize a probability matrix\n",
    "prob_matrix = np.zeros((len(words), len(words)))\n",
    "\n",
    "# Fill the probability matrix\n",
    "for (w1, w2), count in bigram_freq.items():\n",
    "    i = words.index(w1)\n",
    "    j = words.index(w2)\n",
    "    prob_matrix[i][j] = count / word_freq[w1]  # Probability of w2 given w1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>am</th>\n",
       "      <th>khan</th>\n",
       "      <th>salman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>khan</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salman</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          i   am  khan  salman\n",
       "i       0.0  1.0   0.0     0.0\n",
       "am      0.0  0.0   0.0     1.0\n",
       "khan    0.0  1.0   0.0     0.0\n",
       "salman  0.0  0.0   0.5     0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print count_matrix as dataframe with words as index and columns\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(prob_matrix, index=words, columns=words)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Obtaining dependency information for openpyxl from https://files.pythonhosted.org/packages/6a/94/a59521de836ef0da54aaf50da6c4da8fb4072fb3053fa71f052fd9399e7a/openpyxl-3.1.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Obtaining dependency information for et-xmlfile from https://files.pythonhosted.org/packages/96/c2/3dd434b0108730014f1b96fd286040dc3bcb70066346f7e01ec2ac95865f/et_xmlfile-1.1.0-py3-none-any.whl.metadata\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "   ---------------------------------------- 0.0/250.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 250.0/250.0 kB 16.0 MB/s eta 0:00:00\n",
      "Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use IMDB dataset and apply multinomial naive bayes and then use the model to predict the sentiment of the review\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the xlsx file as a DataFrame\n",
    "df = pd.read_excel('Dataset C1.xlsx')\n",
    "# preprocess each review by removing special characters and punctuations and then updated the dataframe\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['type'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Multinomial Naive Bayes on the train dataset\n",
    "vectorizer = CountVectorizer() # Convert a collection of text documents to a matrix of token counts\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.8942307692307693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'entertainment'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the sentiment of the test dataset\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Model accuracy:', accuracy)\n",
    "\n",
    "#use the model to predict the sentiment of the review\n",
    "text = \"This movie was amazing! I loved every minute of it.\"\n",
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "text = text.lower()\n",
    "text_vector = vectorizer.transform([text])\n",
    "type = model.predict(text_vector)[0]\n",
    "type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sports', 'entertainment']\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['type'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Bag of Words matrix using X_train and Y_train without using vectorizer\n",
    "words = []\n",
    "for review in X_train:\n",
    "    words.extend(review.split())\n",
    "words = list(set(words))\n",
    "\n",
    "# Initialize a bow matrix with rows as the number of unique sentiments and columns as the number of unique words\n",
    "sentiments = y_train.unique().tolist()  # Get unique sentiments\n",
    "print(sentiments)\n",
    "bow_matrix = np.zeros((len(y_train.unique()), len(words)))\n",
    "# Fill the bag of words matrix using the frequency of each word in the class of y_train\n",
    "for i, sentiment in enumerate(sentiments):\n",
    "    sentiment_words = ' '.join(X_train[y_train == sentiment])\n",
    "    for j, word in enumerate(words):\n",
    "        bow_matrix[i, j] = sentiment_words.count(word)\n",
    "\n",
    "# Generate a conditional probability matrix where the (i, j) element is the probability of word j given sentiment i and use Laplace smoothing with alpha = 1\n",
    "alpha = 1\n",
    "prob_matrix = (bow_matrix + alpha) / (bow_matrix.sum(axis=1)[:, None] + alpha * len(words))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.9134615384615384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sports'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the sentiment of the test dataset using the conditional probability matrix and the chain rule of probability\n",
    "y_pred = []\n",
    "for review in X_test:\n",
    "    review_words = review.split()\n",
    "    probs = []\n",
    "    for i, sentiment in enumerate(sentiments):\n",
    "        #initial value of probability is the prior probability of the sentiment\n",
    "        prob = len(X_train[y_train == sentiment]) / len(X_train)\n",
    "        for word in review_words:\n",
    "            if word in words:\n",
    "                j = words.index(word)\n",
    "                prob *= prob_matrix[i, j]\n",
    "        probs.append(prob)\n",
    "    y_pred.append(sentiments[np.argmax(probs)])\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Model accuracy:', accuracy)\n",
    "\n",
    "#use the model to predict the sentiment of the review\n",
    "text = \"This cricket match was amazing! I loved every minute of it.\"\n",
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "text = text.lower()\n",
    "text_words = text.split()\n",
    "probs = []\n",
    "for i, sentiment in enumerate(sentiments):\n",
    "    prob = len(X_train[y_train == sentiment]) / len(X_train)\n",
    "    for word in text_words:\n",
    "        if word in words:\n",
    "            j = words.index(word)\n",
    "            prob *= prob_matrix[i, j]\n",
    "    probs.append(prob)\n",
    "type = sentiments[np.argmax(probs)]\n",
    "type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n e w </w>': 2, 'w i d e r </w>': 3, 'n e w e r </w>': 6, 'l o w e s t </w>': 2, 'l o w </w>': 5}\n"
     ]
    }
   ],
   "source": [
    "#random text\n",
    "import re \n",
    "\n",
    "\n",
    "text = \"new new wider wider wider newer newer newer newer newer newer lowest lowest low low low low low\"\n",
    "#remove punctuations\n",
    "text = re.sub(r'[^\\w\\s]','',text)\n",
    "vocab = {}\n",
    "for word in text.split():\n",
    "    new_word = ' '.join(list(word))+ ' </w>'\n",
    "    if new_word in vocab:\n",
    "        vocab[new_word] += 1\n",
    "    else:\n",
    "        vocab[new_word] = 1\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pairs(vocab):\n",
    "    pairs = {}\n",
    "    for word, freq in vocab.items():\n",
    "        elems = word.split()\n",
    "        for i in range(len(elems) - 1):\n",
    "            if (elems[i], elems[i + 1]) in pairs:\n",
    "                pairs[(elems[i], elems[i + 1])] += freq\n",
    "            else:\n",
    "                pairs[(elems[i], elems[i + 1])] = freq\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_pair(pairs):\n",
    "    for pair, freq in pairs.items():\n",
    "        if freq == max(pairs.values()):\n",
    "            return list(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(vocab, merge_pair):\n",
    "    new_vocab = {}\n",
    "    pat = ' '.join(list(merge_pair))\n",
    "    regex = re.compile(r'(?<!\\S)' + pat + r'(?!\\S)')\n",
    "    for word in vocab:\n",
    "        new_word = regex.sub(''.join(merge_pair), word)\n",
    "        new_vocab[new_word] = vocab[word]\n",
    "    return new_vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule : e r Epoch 1: {'n e w </w>': 2, 'w i d er </w>': 3, 'n e w er </w>': 6, 'l o w e s t </w>': 2, 'l o w </w>': 5}\n",
      "Rule : er </w> Epoch 2: {'n e w </w>': 2, 'w i d er</w>': 3, 'n e w er</w>': 6, 'l o w e s t </w>': 2, 'l o w </w>': 5}\n",
      "Rule : n e Epoch 3: {'ne w </w>': 2, 'w i d er</w>': 3, 'ne w er</w>': 6, 'l o w e s t </w>': 2, 'l o w </w>': 5}\n",
      "Rule : ne w Epoch 4: {'new </w>': 2, 'w i d er</w>': 3, 'new er</w>': 6, 'l o w e s t </w>': 2, 'l o w </w>': 5}\n",
      "Rule : l o Epoch 5: {'new </w>': 2, 'w i d er</w>': 3, 'new er</w>': 6, 'lo w e s t </w>': 2, 'lo w </w>': 5}\n",
      "Rule : lo w Epoch 6: {'new </w>': 2, 'w i d er</w>': 3, 'new er</w>': 6, 'low e s t </w>': 2, 'low </w>': 5}\n",
      "Rule : new er</w> Epoch 7: {'new </w>': 2, 'w i d er</w>': 3, 'newer</w>': 6, 'low e s t </w>': 2, 'low </w>': 5}\n",
      "Rule : low </w> Epoch 8: {'new </w>': 2, 'w i d er</w>': 3, 'newer</w>': 6, 'low e s t </w>': 2, 'low</w>': 5}\n",
      "Rule : w i Epoch 9: {'new </w>': 2, 'wi d er</w>': 3, 'newer</w>': 6, 'low e s t </w>': 2, 'low</w>': 5}\n",
      "Rule : wi d Epoch 10: {'new </w>': 2, 'wid er</w>': 3, 'newer</w>': 6, 'low e s t </w>': 2, 'low</w>': 5}\n",
      "Rule : wid er</w> Epoch 11: {'new </w>': 2, 'wider</w>': 3, 'newer</w>': 6, 'low e s t </w>': 2, 'low</w>': 5}\n",
      "Rule : new </w> Epoch 12: {'new</w>': 2, 'wider</w>': 3, 'newer</w>': 6, 'low e s t </w>': 2, 'low</w>': 5}\n",
      "Rule : low e Epoch 13: {'new</w>': 2, 'wider</w>': 3, 'newer</w>': 6, 'lowe s t </w>': 2, 'low</w>': 5}\n",
      "Rule : lowe s Epoch 14: {'new</w>': 2, 'wider</w>': 3, 'newer</w>': 6, 'lowes t </w>': 2, 'low</w>': 5}\n",
      "Rule : lowes t Epoch 15: {'new</w>': 2, 'wider</w>': 3, 'newer</w>': 6, 'lowest </w>': 2, 'low</w>': 5}\n",
      "Rule : lowest </w> Epoch 16: {'new</w>': 2, 'wider</w>': 3, 'newer</w>': 6, 'lowest</w>': 2, 'low</w>': 5}\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "test_text = \"new newer lowest wider wider\"\n",
    "test_text = re.sub(r'[^\\w\\s]','',test_text)\n",
    "test_vocab = {}\n",
    "for word in test_text.split():\n",
    "    new_word = ' '.join(list(word))+ ' </w>'\n",
    "    if new_word in test_vocab:\n",
    "        test_vocab[new_word] += 1\n",
    "    else:\n",
    "        test_vocab[new_word] = 1\n",
    "\n",
    "list_rules = []\n",
    "for epoch in range(epochs):\n",
    "    pairs = find_pairs(vocab)\n",
    "    if(len(pairs) == 0):\n",
    "        break\n",
    "    best_pair = find_best_pair(pairs)\n",
    "    list_rules.append(best_pair)\n",
    "    rule = ' '.join(list(best_pair))\n",
    "    print(f'Rule : {rule}', end = \" \")\n",
    "    vocab = merge(vocab, best_pair)\n",
    "    print(f'Epoch {epoch + 1}: {vocab}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule : e r {'n e w </w>': 1, 'n e w er </w>': 1, 'l o w e s t </w>': 1, 'w i d er </w>': 2}\n",
      "Rule : er </w> {'n e w </w>': 1, 'n e w er</w>': 1, 'l o w e s t </w>': 1, 'w i d er</w>': 2}\n",
      "Rule : n e {'ne w </w>': 1, 'ne w er</w>': 1, 'l o w e s t </w>': 1, 'w i d er</w>': 2}\n",
      "Rule : ne w {'new </w>': 1, 'new er</w>': 1, 'l o w e s t </w>': 1, 'w i d er</w>': 2}\n",
      "Rule : l o {'new </w>': 1, 'new er</w>': 1, 'lo w e s t </w>': 1, 'w i d er</w>': 2}\n",
      "Rule : lo w {'new </w>': 1, 'new er</w>': 1, 'low e s t </w>': 1, 'w i d er</w>': 2}\n",
      "Rule : new er</w> {'new </w>': 1, 'newer</w>': 1, 'low e s t </w>': 1, 'w i d er</w>': 2}\n",
      "Rule : low </w> {'new </w>': 1, 'newer</w>': 1, 'low e s t </w>': 1, 'w i d er</w>': 2}\n",
      "Rule : w i {'new </w>': 1, 'newer</w>': 1, 'low e s t </w>': 1, 'wi d er</w>': 2}\n",
      "Rule : wi d {'new </w>': 1, 'newer</w>': 1, 'low e s t </w>': 1, 'wid er</w>': 2}\n",
      "Rule : wid er</w> {'new </w>': 1, 'newer</w>': 1, 'low e s t </w>': 1, 'wider</w>': 2}\n",
      "Rule : new </w> {'new</w>': 1, 'newer</w>': 1, 'low e s t </w>': 1, 'wider</w>': 2}\n",
      "Rule : low e {'new</w>': 1, 'newer</w>': 1, 'lowe s t </w>': 1, 'wider</w>': 2}\n",
      "Rule : lowe s {'new</w>': 1, 'newer</w>': 1, 'lowes t </w>': 1, 'wider</w>': 2}\n",
      "Rule : lowes t {'new</w>': 1, 'newer</w>': 1, 'lowest </w>': 1, 'wider</w>': 2}\n",
      "Rule : lowest </w> {'new</w>': 1, 'newer</w>': 1, 'lowest</w>': 1, 'wider</w>': 2}\n"
     ]
    }
   ],
   "source": [
    "for rule in list_rules:\n",
    "    # pairs = find_pairs(test_vocab)\n",
    "    curr_rule = ' '.join(list(rule))\n",
    "    print(f'Rule : {curr_rule}', end = \" \")\n",
    "    test_vocab = merge(test_vocab, rule)\n",
    "    print(f'{test_vocab}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words as tokens\n",
    "#write a random paragraph\n",
    "\n",
    "paragraph = \"I am a student of computer science and engineering. I am currently studying at the University of Asia Pacific. I am a very good student. I am very much interested in programming. I am also interested in machine learning. I am also interested in data science. I am also interested in artificial intelligence. I am also interested in deep learning. I am also interested in computer vision. I am also interested in natural language processing. I am also interested in robotics. I am also interested in computer graphics. I am also interested in computer networks. I am also interested in cyber security. I am also interested in web development. I am also interested in mobile application development. I am also interested in game development. I am also interested in software development. I am also interested in hardware development. I am also interested in computer architecture. I am also interested in computer organization. I am also interested in computer design. I am also interested in computer engineering. I am also interested in computer science\"\n",
    "\n",
    "#remove punctuations from the paragraph\n",
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "for x in paragraph.lower():\n",
    "    if x in punctuations:\n",
    "        paragraph = paragraph.replace(x, \"\")\n",
    "\n",
    "paragraph = paragraph.lower()\n",
    "#remove all the spaces from the paragraph\n",
    "paragraph = paragraph.replace(\" \", \"\")\n",
    "\n",
    "words = []\n",
    "lowerTokensSet = set([x.lower() for x in tokens.words()])\n",
    "i = 0\n",
    "while i < len(paragraph):\n",
    "    mWord = \"\"\n",
    "    for j in range(i, len(paragraph)):\n",
    "        temp = paragraph[i:j+1]\n",
    "        temp = temp.lower()\n",
    "        if temp in lowerTokensSet and len(temp) > len(mWord):\n",
    "            mWord = temp\n",
    "    i = i+len(mWord)\n",
    "    words.append(mWord)\n",
    "\n",
    "newParagraph = \" \".join(words)\n",
    "print(newParagraph)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
